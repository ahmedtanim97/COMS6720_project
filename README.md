# COMS6720_project
### Prerequisites

1. Python 3.11.5 environment
2. Required packages installed (see `requirements.txt`)
3. OpenAI API key set in your environment variables or .env file
4. PDF documents in the data directory

### Setup

1. Clone the repository and navigate to the project root:
   ```bash
   git clone git@github.com:ahmedtanim97/COMS6720_project.git
   cd COMS6720_project
   ```

2. Create and activate virtual environment:
   ```bash
   python -m venv <environment_name>
   <environment_name>\Scripts\activate
   ```


3. Install the required dependencies:
   ```bash
   pip install -r requirements.txt
   ```

4. Create a `.env` file in the project root with your OpenAI API key:
   ```
   OPENAI_API_KEY=your_api_key_here
   ```

# Instructions for Vanilla Rag

A sophisticated document processing and retrieval system that leverages FAISS (Facebook AI Similarity Search) and OpenAI embeddings to create an interactive question-answering interface for coral research documents.

# Project Structure
```
project/
├── scripts/
│   └── FAISS_scripts/
│       ├── item_01_database_creation_FAISS.py    # Initial document processing
│       ├── item_02_generate_citations_APA_FAISS.py # Citation generation
│       ├── item_03_replace_source_by_citation.py   # Citation integration
│       ├── item_04_retriever_FAISS.py             # Query processing
│       ├── item_05_streamlit_FAISS.py             # Web interface
│       ├── item_06_eval_01_save_response_and_context.py # Evaluation data collection
│       ├── item_07_eval_02_human_evaluation.py     # System evaluation
│       ├── item_08_eval_03_generate_questions_answers_from_chunk.py  
│       ├── item_09_eval_04_save_response_and_context_LLM.py
│       ├── item_10_eval_05_llm_evaluation.py
├── data/
│   └── pdfs/            # Source PDF documents
├── faiss_index/             # FAISS vector database storage
├── additional_files/
│   ├── citations.csv        # Generated APA citations
│   ├── Q&A-Human_generated.csv # Human-created test questions
│   ├── Q&A-human_generated_with_context.csv # System responses
│   ├── Q&A_result-human_generated.csv # Detailed evaluation results
│   ├── overall_result-human_generated.csv # Summary evaluation metrics
│   ├── Q&A-LLM_generated.csv # LLM-created test questions
│   ├── Q&A-LLM_generated_with_context.csv # System responses
│   ├── Q&A_result-LLM_generated.csv # Detailed evaluation results
│   ├── overall_result-LLM_generated.csv # Summary evaluation metrics
|__

### Input Files
Located in `data/pdfs/`:
- Collection of PDF documents containing research papers
- Used by `item_01_database_creation_FAISS.py` for initial database creation-
- Database will be stored at faiss_index/

### Generated Files
Located in `additional_files/`:

1. **citations.csv**
   - Generated by: `item_02_generate_citations_APA_FAISS.py`
   - Contains: PDF source paths and their corresponding APA citations
   
2. Updated **citations.csv**
   - Generated by: `item_03_replace_source_by_citations.py`
   - Replaces source file paths with proper APA citations which are used in the vector database.
   
3. **Q&A-Human_generated.csv**
   - Used by: `item_06_eval_01_save_response_and_context.py`
   - Contains: Human-generated questions and their ground truth answers
   

4. **Q&A-human_generated_with_context.csv**
   - Generated by: `item_06_eval_01_save_response_and_context.py`
   - Contains: Original Q&A pairs, system responses, and relevant context chunks
   

5. **Q&A_result-human_generated.csv**
   - Generated by: `item_07_eval_02_human_evaluation.py`
   - Contains: Detailed evaluation metrics for each question
   - Metrics include:
     - Context Precision and Recall
     - Faithfulness
     - Answer Relevancy
     - Answer Similarity
     

6. **overall_result-human_generated.csv**
   - Generated by: `item_07_eval_02_human_evaluation.py`
   - Contains: Aggregated evaluation metrics for the entire system
   - Average scores for all evaluation metrics

6. **Q&A-LLM_generated.csv**
   - Generated by: `item_08_eval_03_generate_questions_answers_from_chunk.py`
   - Contains: LLM-generated questions and their corresponding answers
   
7. **Q&A-LLM_generated_with_context.csv**
   - Generated by: `item_09_eval_04_save_response_and_context_LLM.py`
   - Contains: LLM-generated Q&A pairs with context
   

8. **Q&A_result-LLM_generated.csv**
   - Generated by: `item_10_eval_05_llm_evaluation.py`
   - Contains: Detailed evaluation metrics for LLM-generated Q&A pairs
   - Metrics include:
     - Context Precision
     - Context Recall
     - Faithfulness
     - Answer Relevancy
     - Answer Similarity

9. **overall_result-LLM_generated.csv**
   - Generated by: `item_10_eval_05_llm_evaluation.py`
   - Contains: Aggregated evaluation metrics for LLM-generated Q&A pairs

## For running script
```bash
python scripts/FAISS_scripts/*
```

In place of * please use the following execution order

## Order for executing script
1. item_01_database_creation_FAISS.py
2. item_02_generate_citations_APA_FAISS.py
3. item_03_replace_source_by_citation.py
4. item_06_eval_01_save_response_and_context.py
5. item_07_eval_02_human_evaluation.py
6. item_08_eval_03_generate_questions_answers_from_chunk.py
7. item_09_eval_04_save_response_and_context_LLM.py
8. item_10_eval_05_llm_evaluation.py

### 1. Database Creation
```bash
python scripts/FAISS_scripts/item_01_database_creation_FAISS.py
```
This script:
- Loads PDF documents from the specified directory
- Splits text into manageable chunks
- Creates FAISS vector embeddings using OpenAI
- Saves the vector database locally

### 2. Generate APA Citations
```bash
python scripts/FAISS_scripts/item_02_generate_citations_APA_FAISS.py
```
This script:
- Processes the vector database
- Generates APA citations using GPT-4
- Saves citations to a CSV file

### 3. Update Source References
```bash
python scripts/FAISS_scripts/item_03_replace_source_by_citation.py
```
This script replaces source file paths with proper APA citations in the vector database.

### 5. Run System Evaluation
```bash
# Generate system responses for evaluation
python scripts/FAISS_scripts/item_06_eval_01_save_response_and_context.py

# Perform system evaluation
python scripts/FAISS_scripts/item_07_eval_02_human_evaluation.py
```

Similarly you need to execute the rest of the execution in the order

Evaluation results are saved in:
- `Q&A_result-human_generated.csv`: Detailed results for each question
- `overall_result-human_generated.csv`: Summary metrics for system performance


# Chain of Thought

This README provides documentation for the hybrid retrieval system scripts and their associated test files. The hybrid system combines vector-based (FAISS) and graph-based (NetworkX) retrieval for more comprehensive coral research information retrieval.

## Overview of Chain of Thought

The hybrid scripts in this project implement a sophisticated information retrieval system combining semantic search with graph-based knowledge representation.

### Key Scripts

1. **item_01_database_creation_Hybrid.py**
   - Creates a hybrid database with both FAISS vector index and NetworkX graph components
   - Extracts domain-specific entities from coral research documents

2. **item_02_generate_citation_APA_FAISS.py**
   - Generates APA citations for all documents stored in the FAISS index
   - Creates a CSV file mapping document sources to their corresponding APA citations

3. **item_03_replace_source_by_citation.py**
   - Updates document metadata in the FAISS index to use APA citations
   - Replaces raw source paths with properly formatted academic citations

4. **item_04_CoT_search.py**
   - Implements a single search (single call to LLM) which costs less.
   - Features chain of tought reasoning capability using the hybrid retrieval system

   

## Running the Hybrid Scripts

To use these scripts, follow the instructions below. Make sure you have all required dependencies installed.

### Running the Scripts

**1. Create the Hybrid Database:**
```bash
python scripts/Hybrid_scripts/item_01_database_creation_Hybrid.py
```
This will:
- Process PDF files in the data directory
- Create a FAISS vector index
- Build a NetworkX graph connecting documents and entities
- Save both to the hybrid_index directory

**2. Generate APA Citations:**
```bash
python scripts/Hybrid_scripts/item_02_generate_citation_APA_FAISS.py
```
This will:
- Process documents in the FAISS index
- Generate APA format citations for each document
- Save a mapping CSV file in additional_files/citations.csv

**3. Replace Sources with Citations:**
```bash
python scripts/Hybrid_scripts/item_03_replace_source_by_citation.py
```
This will:
- Update the FAISS index with proper citations
- Replace raw file paths with academic citations

**4. Run CoT search (For Basic Search):**
```bash
python scripts/Hybrid_scripts/item_04_CoT_search.py
```
This will:
- Return 3 variable final_answer, reasoning_steps, citations by calling "answer_to_QA(query)" function

**4. Run CoT search (For Basic Search):**

### Script Execution Order

For proper functionality, execute the scripts in this order:
1. item_01_database_creation_Hybrid.py
2. item_02_generate_citation_APA_FAISS.py
3. item_03_replace_source_by_citation.py
4. item_04_CoT_search.py
5. A file "retrieved_chunks.xlsx" file is generated upon execution from where we manually put the chunks and the answer in the Q&A-human_generated_with_context csv files and evalauated based on the "scripts/FAISS_scripts/item_07_eval_02_human_evaluation.py". We had to do this due to lack of time and this works because the evalaution is the same based on ground truth, generated answers and the retrieved chunks.

## Data Flow

1. **Document Processing**
   ```
   PDFs → database_creation_FAISS.py → FAISS index / Graph  
   FAISS index → generate_citations_APA_FAISS.py → citations.csv
   citations.csv + FAISS index → replace_source_by_citation.py → Updated FAISS index
   ```

2. **Query Processing**
   ```
   User Query → CoT_search.py → Answer with citations
   ```
## Project Structure
```
project/
├── scripts/
│   └── FAISS_scripts/
│       ├── item_01_database_creation_FAISS.py            # Initial document processing
│       ├── item_02_generate_citations_APA_FAISS.py       # Citation generation
│       ├── item_03_replace_source_by_citation.py         # Citation integration
│       ├── item_04_retriever_FAISS.py                    # Query processing
│       ├── item_05_streamlit_FAISS.py                    # Web interface (not implemented)
│       ├── item_06_eval_01_save_response_and_context.py  # Evaluation data collection
│       ├── item_07_eval_02_human_evaluation.py           # System evaluation
│       ├── item_08_eval_03_generate_questions_answers_from_chunk.py  
│       ├── item_09_eval_04_save_response_and_context_LLM.py
│       ├── item_10_eval_05_llm_evaluation.py
│   └── Hybrid_scripts/
│       ├── item_01_database_creation_FAISS.py        # Initial document processing
│       ├── item_02_generate_citations_APA_FAISS.py   # Citation generation
│       ├── item_03_replace_source_by_citation.py     # Citation integration
│       ├── item_04_CoT_search.py                     # Query processing with CoT
│      
├── data/
│   └── pdfs/           # Source PDF documents
│
├── faiss_index/        # faiss vector database storage
│   ├── index.faiss            
│   ├── index.pkl  
│
├── hybrid_index/            # Hybrid vector database storage
│   ├── index.faiss            
│   ├── index.pkl        
│   ├── networkx_graph.pkl 
|
├── additional_files/
│   ├── citations.csv                    # Generated APA citations
│   ├── Q&A-Human_generated.csv          # Human-created test questions
│   ├── Q&A-human_generated_with_context.csv       # System responses
│   ├── Q&A_result-human_generated.csv             # Detailed evaluation results
│   ├── overall_result-human_generated.csv         # Summary evaluation metrics
│   ├── Q&A-LLM_generated.csv                      # LLM-created test questions
│   ├── Q&A-LLM_generated_with_context.csv         # System responses
│   ├── Q&A_result-LLM_generated.csv               # Detailed evaluation results
│   ├── overall_result-LLM_generated.csv           # Summary evaluation metrics
│
└── README.md
```

## Troubleshooting

1. **API key errors:**
   Check that your .env file is properly set up with your OpenAI API key.
